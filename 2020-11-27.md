# 準備
## 言語モデル
* 単語（or 語彙の要素token）の系列生じる確率は、同時分布で書ける。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+P%5Bw_%7B0%7D%2Cw_%7B1%7D%2C...%2Cw_%7B512%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = P[w_{0},w_{1},...,w_{512}] 
\end{equation}">


* 今まで生じた系列から次の単語(or token)の生起確率は、条件付き分布で書ける

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+P%5Bw_%7Bn%7D%7Cw_%7B0%7D%2Cw_%7B1%7D%2C...%2Cw_%7Bn-1%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = P[w_{n}|w_{0},w_{1},...,w_{n-1}] 
\end{equation}">


* Chaine Rule

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+%5Cprod_%7Bi%7D+P%5Bw_%7Bi%7D%7Cw_%7B0%7D%2Cw_%7B1%7D%2C...%2Cw_%7Bi-1%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = \prod_{i} P[w_{i}|w_{0},w_{1},...,w_{i-1}] 
\end{equation}">

* Bi Gramの言語モデル = Markov性の仮定

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+%5Cprod_%7Bi%7D+P%5Bw_%7Bi%7D%7Cw_%7Bi-1%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = \prod_{i} P[w_{i}|w_{i-1}] 
\end{equation}">

* N階のMarkov性からN-Gramを作れるが、実際の言語の構造とは異なっていることが多い
    * 離れた位置の単語どうしに関連性がある場合
   
    　　　　![Screenshot from 2020-11-23 21-16-46](https://user-images.githubusercontent.com/19440811/99961333-953ab700-2dd1-11eb-8656-4a4babd6c12f.png)

* 参考：古典的な作り方
  * https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf

## 言語モデルの評価指標
### PerplexityとEntropy
* 確率分布pにつき以下で定義される。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+PP%28p%29+%3A%3D+2%5E%7BH%28p%29%7D%3D2%5E%7B-%5Csum_x+p%28x%29%5Clog_2+p%28x%29%7D%0A" 
alt="PP(p) := 2^{H(p)}=2^{-\sum_x p(x)\log_2 p(x)}
">

* 例：
  * １：一様分布にxをとると最大のperplexity＝＞でたらめにキーボードを打つみたいな状況
  * ２：one-hotの分布（離散値）をとると最小のperplexity＝＞ここを目指して最適化する

* 上記の式のxは、語彙(vocab)であるが、単語とは限らない（部分文字列など）
* 類似する分布（意味は関係なく、確率上は）が１種類の記号に対応しているほうが実はよい。
  * 例えば、正規化：x_0=`１年生`（全角）,x_1=`1年生`（半角）が上記のx全体からなる集合の要素とする。
  * このとき、アラビア数字と漢数字で意味の違いや明白な用法の違いがなければ、最小のperpleixtyでななくなる。
  * x_0, x_1以外は、ゼロになっても、x_0,x_1の片方がゼロにならない。
  * 正規化だけでなくx_2=「りんご」x_3=「リンゴ」なども同様
* とはいえ、実際のコーパスでは、このような一意性は、保てないので理想の言語モデルでも最小のperplexityにはなりえない。

## Tokenizer
* 文章の文字列を入力して、部分文字列のリストを出力する関数
* 事前学習モデルの動作は、このtokenizerの語彙ファイルに依存している。
* 人間が決めたtokenへ分割
  * スペースで区切るだけ（英語など）
  * スペースがない言語（日本語など）
    * 形態素解析が必要：mecab, jumanとか
* アルゴリズムでtokenとなる部分文字列を決めながら分割
  * BPE
    * コーパス上でレアな単語を頻出の部分文字列へ分割する
    * 日本語の場合は、形態素を英語の「単語」扱いして分割
    * 読売巨人軍 = [読売, 巨人, 軍]
      * 読売＝「読売新聞」などがあるので、より高頻度
      * 巨人＝「進撃の巨人」などがあり、野球球団は「巨人」「ジャイアンツ」などの用法が多い
      * 軍＝「米軍」などがある
      * 「トピック」（メディア、スポーツ、エンタメ、政治）などは、人間にとって重要でもperplexityの最適化の観点からすると重要ではない。
  * sentencepiece
    * 「空白」を文字とみなして、部分文字列へ分割する。
    * 日本語・タイ語などの文章を生の文字列から形態素解析を経由しないで直接分割する
    
## Transformer Block
### 一行要約
* Transformerの入力のtoken系列から、token系列の内容と位置の統計的記憶（memory adressing）の学習を行う。

### 導入
* 例を観察してみる：
  * appleという記号が６番目にgreenという記号が３番目にある。（かかり受けという関係は無視）
  * greenという記号が５番目にeatingという記号が３番目にある。（述語と目的語という意味は無視）
  * 問題意識：
    * 絶対位置IにあるXという記号と絶対位置JにあるYという記号が出現する確率を計算してみてはどうだろうか？
  
    ![Screenshot from 2020-11-23 21-16-46](https://user-images.githubusercontent.com/19440811/99961333-953ab700-2dd1-11eb-8656-4a4babd6c12f.png)

* 問題意識をTransformerの文脈で考える。
  * １：I番目やJに番目の位置という情報はどのように表現しているだろうか？
    * Positional Encoding：tokenの位置を特徴量エンジニアリング的な計算で与える。
  * ２：XとYという２個の記号の組み合わせの関係を取得しているモジュールは何か？
    * Scaled Dot Product Attention：Key&Val のHashing (pythonのdictと類似の処理) をパラメータの行列で行う
  * ３：複数の関係性（Bi-Gram＋かかりうけ＋述語目的語＋その他の人間の知識に存在しない関係など）を同時に学習ができないだろうか？
    * Multi Head Attention：上記１＆２からなる記憶モジュールを複数並列に推論した出力をcancatし線形レイヤーで統合する。

## Positional Encoding
  * 以下の式で行う
    * なぜ、この式が上手く行くのかの解釈は要検討
    * 隣接する位置の値は、正負が異なっていることは保証できている
      * sin(), cos()の定義域は1以下になっていそう、分母が巨大なので

  ![Screenshot from 2020-11-24 01-57-11](https://user-images.githubusercontent.com/19440811/99992625-e8286480-2df9-11eb-8119-230274299ca0.png)

### Scaled Dot Attention
* Scaled Dot Product Attention
  * 以下の式で表現される。
    * ![Screenshot from 2020-11-24 02-28-14](https://user-images.githubusercontent.com/19440811/99994721-bb298100-2dfc-11eb-8ded-94fc36077d8f.png)

  * queryとして、絶対位置Iと単語Xが足されたベクトルQ
  * python-dictを使った不正確なアナロジー説明
    ```python
    Q = "14番目+リンゴ" 
    # 実際は位置の特徴量ベクトルと「リンゴ」の埋め込みベクトルの和
    # Kとdot productをする関数は、Qを受け取ってdictを返す関数とみなす。
    K_dot_product(Q) = {"50番目+リンゴ":"Qと<50番目+リンゴ>の内積値", 
                        "16番目+東京":"Qと<16番目+東京>の内積値", 
                        "100番目+魚":"Qと<100番目+魚>の内積値", 
                        "14番目+みかん":"Qと<14番目+みかん>の内積値",
                        ...}
    # keyがtoken位置の個数xtokenizerの語彙数くらいあるとする。
    ```
    * 上記のアナロジーの例では、Key Errorは起きない
      * keyベクトルと内積をとる関数は、「アクセスのしかた」だけしか与えていない。
    * softmaxをとると、{"50番目+リンゴ":0.4, "16番目+東京":0.0, "100番目+魚":0.0, "14番目+みかん":0.4, ... }のようになることが望まれる。 
    * 「みかん」は「りんご」とは内容が異なるが位置が同じなら置き換えても良さそうであるので。
  * softmaxのtoken内容の分布ベクトルがどの内容へ「読み出す」かを決定する。
  * 分布ベクトルとValueのベクトルの次元ごとの掛け算(position wise multiplication)してValueの内容を読み出す
  * softmaxを取る前に割り算(Scaled)しているのは、数値計算した勾配が小さくなりすぎることを防ぐため。

  <img src="https://user-images.githubusercontent.com/19440811/99990803-a0084280-2df7-11eb-820f-f50b6e8378d9.png" width="300" height="300">

## Multi Head Attention
* 以下の数式
  * Scaled Dot Attentionの入力をする前に線形レイヤーに通しているのは、次元を削減するため。headの個数で割った次元になる。

  ![Screenshot from 2020-11-24 19-31-28](https://user-images.githubusercontent.com/19440811/100082575-beb81900-2e8b-11eb-8874-dbe6d49f505f.png)



  <img src="https://user-images.githubusercontent.com/19440811/99990812-a26a9c80-2df7-11eb-99d0-c706ca8ee0ba.png" width="300" height="300">
  
### まとめてみると

![Screenshot from 2020-11-24 01-47-38](https://user-images.githubusercontent.com/19440811/99990383-16f10b80-2df7-11eb-8920-e3666e6caa6b.png)

## GPT-1
