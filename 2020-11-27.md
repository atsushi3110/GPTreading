# 準備
## 言語モデル
* 単語（or 語彙の要素token）の系列生じる確率は、同時分布で書ける。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+P%5Bw_%7B0%7D%2Cw_%7B1%7D%2C...%2Cw_%7B512%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = P[w_{0},w_{1},...,w_{512}] 
\end{equation}">


* 今まで生じた系列から次の単語(or token)の生起確率は、条件付き分布で書けるので、Chaine Ruleより同時分布も条件付き確率で書ける。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+%5Cprod_%7Bi%7D+P%5Bw_%7Bi%7D%7Cw_%7B0%7D%2Cw_%7B1%7D%2C...%2Cw_%7Bi-1%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = \prod_{i} P[w_{i}|w_{0},w_{1},...,w_{i-1}] 
\end{equation}">

* Bi-gramの言語モデルは、上記の等式で1階のMarkov性の仮定を置くと得られる。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Bequation%7D%0AP%5Bw%5D+%3D+%5Cprod_%7Bi%7D+P%5Bw_%7Bi%7D%7Cw_%7Bi-1%7D%5D+%0A%5Cend%7Bequation%7D" 
alt="\begin{equation}
P[w] = \prod_{i} P[w_{i}|w_{i-1}] 
\end{equation}">

* N階のMarkov性からN-ramを作れるが、実際の言語の構造とは異なっていることが多い
    * 離れた位置の単語どうしに関連性がある場合
   
    　　　　![Screenshot from 2020-11-23 21-16-46](https://user-images.githubusercontent.com/19440811/99961333-953ab700-2dd1-11eb-8656-4a4babd6c12f.png)

* 参考：古典的な作り方
  * https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf

## 言語モデルの評価指標
### PerplexityとEntropy
* 確率分布pにつき以下で定義される。
  * 指数の肩(exponent)がShannonのEntropyである。

      <img src=
      "https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+PP%28p%29+%3A%3D+2%5E%7BH%28p%29%7D%3D2%5E%7B-%5Csum_x+p%28x%29%5Clog_2+p%28x%29%7D%0A" 
      alt="PP(p) := 2^{H(p)}=2^{-\sum_x p(x)\log_2 p(x)}
      ">

* 例：
  * １：一様分布にxをとると最大のperplexity＝＞でたらめにキーボードを打つみたいな状況
  * ２：one-hotの分布（離散値）をとると最小のperplexity＝＞ここを目指して最適化する

* 上記の式のxは、語彙(vocab)であるが、単語とは限らない（部分文字列など）
* 類似する分布（意味は関係なく、確率上は）が１種類の記号に対応しているほうが実はよい。
  * 例えば、正規化：x_0=`１年生`（全角）,x_1=`1年生`（半角）が上記のx全体からなる集合の要素とする。
  * このとき、アラビア数字と漢数字で意味の違いや明白な用法の違いがなければ、最小のperpleixtyでななくなる。
  * x_0, x_1以外は、ゼロになっても、x_0,x_1の片方がゼロにならない。
  * 正規化だけでなくx_2=「りんご」x_3=「リンゴ」なども同様
* とはいえ、実際のコーパスでは、このような一意性は、保てないので理想の言語モデルでも最小のperplexityにはなりえない。

## Tokenizer
* 文章の文字列を入力して、部分文字列のリストを出力する関数
* 事前学習モデルの動作は、このtokenizerの語彙ファイルに依存している。
* 人間が決めたtokenへ分割
  * スペースで区切るだけ（英語など）
  * スペースがない言語（日本語など）
    * 形態素解析が必要：mecab, jumanとか
* アルゴリズムでtokenとなる部分文字列を決めながら分割
  * BPE
    * コーパス上でレアな単語を頻出の部分文字列へ分割する
    * 日本語の場合は、形態素を英語の「単語」扱いして分割
    * 読売巨人軍 = [読売, 巨人, 軍]に分割
      * 巨人＝ 野球球団は「巨人」「ジャイアンツ」などが高頻度
      * 軍＝「米軍」などどもあり高頻度
      * 「トピック」（メディア、スポーツ、エンタメ、政治）などは、人間にとって重要でもperplexityの最適化の観点からすると重要ではない。
  * sentencepiece
    * 「空白」を文字とみなして、部分文字列へ分割する。
    * 日本語・タイ語などの文章を生の文字列から形態素解析を経由しないで直接分割する
    
## Transformer Block
### 一行要約
* Transformerの入力のtoken系列から、token系列の内容と位置の統計的記憶（memory adressing）の学習を行う。

### 導入
* 例を観察してみる：
  * appleというtokenが６番目にgreenというtokenが３番目にある。（かかり受けという関係は抽象化されている）
  * greenというtokenが５番目にeatingというtokenが３番目にある。（述語と目的語という意味は抽象化されている）
  * 問題意識：
    * 絶対位置IにあるXという記号と絶対位置JにあるYという記号が出現する確率を計算してみてはどうだろうか？
  
    ![Screenshot from 2020-11-23 21-16-46](https://user-images.githubusercontent.com/19440811/99961333-953ab700-2dd1-11eb-8656-4a4babd6c12f.png)

* 問題意識をTransformerの文脈で考える。
  * **１：tokenの位置はどのように表現しているか？**
    * Positional Encoding：tokenの位置をルール計算式またはパラメータ化した埋め込みで与える。
  * **２：２個のtokenのペアの関係(一般には記号の部分集合)を取得しているモジュールは何か？**
    * Scaled Dot Product Attention：Key&Val のHashingをパラメータの行列で行う
      * Hashingを微分可能な式で実行する
  * **３：複数の関係性（一般に人間の明示的知識に存在しない関係など）を同時に学習ができないだろうか？**
    * Multi Head Attention：上記１＆２からなる記憶モジュールを複数(8~16個)並列に推論した出力をcancatし線形レイヤーで統合する。
      * かかりうけ＋述語目的語のような人間が与えられるとは限らない特徴をHashingから複数抽出して合成する
      * レイヤー数によるcompositionalityとは別種のcomposition

### Positional Encoding
  * ルールの場合は、以下の式で行う。iは次元のindexで、posがtokenの位置
    * 例：appleという記号がgreenという記号の２番前にある。
  
  <img src="https://user-images.githubusercontent.com/19440811/99992625-e8286480-2df9-11eb-8119-230274299ca0.png" width="300" height="80">

  * 原論文によると、以下の性質が成立するので、相対距離を推定しやすい。（わかってない）

    <img src=
   "https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0APE_%7B%28pos%2C+i%2Bk%29%7D+%3D+a%5Ccdot+PE_%7B%28pos%2C+i%29%7D+%2B+b%0A%5Cend%7Balign%2A%7D" 
   alt="\begin{align*}
   PE_{(pos, i+k)} = a\cdot PE_{(pos, i)} + b
   \end{align*}">


### Scaled Dot Attention
* Scaled Dot Product Attention
  * 以下の式で表現される。
    ![Screenshot from 2020-11-24 02-28-14](https://user-images.githubusercontent.com/19440811/99994721-bb298100-2dfc-11eb-8ded-94fc36077d8f.png)

  * queryとkeyの内積で「近さ」を測定して、「強弱」の確率分布に変換して、Vに反映させる。
  * n:最大token個数, d_k:埋め込み次元とすると、Q,K,Vは, n × d_k行列
  * python-dictを使ったアナロジー説明
    * 単語単位の比較
    ```python
    q = "14番目+リンゴ" 
    # 実際は位置の特徴量ベクトルと「リンゴ」の埋め込みベクトルの和
    # Kとdot productをする関数は、Qを受け取ってdictを返す関数とみなす。
    trK_dot_prod_(q) = {"50番目+リンゴ":"qと<50番目+リンゴ>の内積値", 
                        "16番目+東京":"qと<16番目+東京>の内積値", 
                        "3番目+パリ":"qと<1番目+パリ>の内積値", 
                        "3番目+で":"qと<100番目+で>の内積値", 
                        "14番目+みかん":"qと<14番目+みかん>の内積値",
                        ...}
    # keyが無限に(あらゆる埋め込みベクトルの個数くらい)あるとする。
    ```
    * query&keyが同一文章となる比較(self attention)
    ```python
    q = "3番目+雨" 
    # 実際は位置の特徴量ベクトルと「雨」の埋め込みベクトルの和
    # Kとdot productをする関数は、Qを受け取ってdictを返す関数とみなす。
    trK_dot_prod_(q) = {"1番目+今日":"qと<1番目+今日>の内積値", 
                        "2番目+は":"qと<2番目+は>の内積値", 
                        "3番目+雨":"qと<3番目+雨>の内積値", 
                        "4番目+が":"qと<4番目+が>の内積値", 
                        "5番目+上がり":"qと<5番目+上がり>の内積値",
                        ...}
    # keyが無限に(あらゆる埋め込みベクトルの個数くらい)あるとする。
    ```
    * 文章の一部と単語の比較
    ```python
    q = "6番目+で" 
    trK_dot_prod_(q) = {"今日,は,雨,が,東京":"qと<今日,は,雨,が,東京>の内積値", 
                        "今日,は,雪,が,香港":"qと<今日,は,雨,が,東京>の内積値", 
                        "今日,は,雨,が,強く":"qと<今日,は,雨,が,東京>の内積値", 
                        "今日,は,雨,が,上がり":"qと<今日,は,雨,が,東京>の内積値", 
                        "今日,は,雪,が,積り":"qと<今日,は,雨,が,東京>の内積値",
                        ...}
    # keyが無限に(あらゆる埋め込みベクトルの個数くらい)あるとする。
    # 「今日は雨が東京」などの文章の途中までの系列の埋め込みベクトル 
    ```
  * softmaxの分布ベクトルが「読み出す」内容＆位置を決定する。
  * 分布ベクトルとVの行列掛け算でVの内容を読み出す
  * softmaxを取る前に割り算(Scaled)しているのは、数値計算した勾配が小さくなりすぎることを防ぐため。

  <img src="https://user-images.githubusercontent.com/19440811/99990803-a0084280-2df7-11eb-820f-f50b6e8378d9.png" width="300" height="300">

### Multi Head Attention
* 以下の数式
  * Scaled Dot Attentionの入力をする前に線形レイヤーに通しているのは、次元を削減するため。headの個数で割った次元になる。

  ![Screenshot from 2020-11-24 19-31-28](https://user-images.githubusercontent.com/19440811/100082575-beb81900-2e8b-11eb-8874-dbe6d49f505f.png)



  <img src="https://user-images.githubusercontent.com/19440811/99990812-a26a9c80-2df7-11eb-99d0-c706ca8ee0ba.png" width="300" height="300">
  
## Transformer Blockを用いたEncoder & Decoder
### Masked Head Attention
* seq2seqと同様にEncoderとDecoderを作成する。
* EncoderにはSource入力系列をそのまま入力する。
* ただし、DecoderにEncoderの出力と、Target入力系列をマスク記号で置換した入力の２つを入力する。
* マスク記号左から順番に真のTarget入力記号に置換する。
* Source入力とTarget入力の例
  * 翻訳：Source 日本語文章　＆Target　英語文章
  * チャットボット：Source ユーザー発話（システム発話） & Target システム発話（ユーザー発話）
  * 要約：Source 記事文 & Target 要約文
* 違いは、マスク記号の有無だけ。
* ResNet同様のskip connection（入力と線形レイヤー埋め込みの和による表現）を最終出力で適用する。

### Encoder


![Screenshot from 2020-11-24 20-47-58](https://user-images.githubusercontent.com/19440811/100090408-794d1900-2e96-11eb-85e9-199d1441076a.png)

### Decoder



![Screenshot from 2020-11-24 20-50-36](https://user-images.githubusercontent.com/19440811/100090597-bc0ef100-2e96-11eb-86be-44197a1d2afa.png)


### Attention is All You Need:まとめてみると

![Screenshot from 2020-11-24 01-47-38](https://user-images.githubusercontent.com/19440811/99990383-16f10b80-2df7-11eb-8920-e3666e6caa6b.png)

# GPT-1
## 背景
* 当時(2018)の背景
  * 転移可能な事前学習モデルが複数あるけど一長一短だった
  * あるタスクでは他より上手く行くけど、別のタスクではダメ
  * LSTM要素があるのでGPU計算が困難
  * 当時の代表的モデル：
    * ELMo：BLSTMベースの事前学習の言語モデル
      * GPT１&BERTに方針が近い(言語モデルのトップにファインチューンレイヤーを積む)
      * BERT論文でモデルの構造が比較対象になっている
    * 機械翻訳：多言語（多数言語）へ翻訳しやすい表現をLSTMで学習
      * 特定のsyntax&grammarに依存しない普遍的なベクトルなら良いという気持ち
    * discourse coherence：一貫性
      * 文章２つの発言順番などでGRUベースのencoderで事前学習
      
* シンプルな自己回帰(Autoregressive)の言語モデルをTransformer Decoderのみで事前学習
  * 教師信号が入力文章自身であるようなSelf Supervised
* 以下の４種類のタスクを単一の事前学習モデルから上手く転移させた。
  * Natural Language Inference
    * ２個の文章(P,Q)が入力され、「含む」(P => Q or Q => P)「矛盾」(P = not Q)「その他」判定
    * 論理包含関係の学習
  * Question Answerin
    * 問題文章と質問文章から解答を選択(マークシート方式)
    * SQuadのような解答文章の範囲を問題文章のインデックスで指定することはなし。
  * Semantic Similarity 
    * 文章の表層ではない意味の類似度計算
    * 例えば「東京から大阪へ行く」「大阪から東京へ行く」は類似度0にする。
  * Text Classification
    * 一般的な文章分類：表層だけで解答可能なものも含む

## 事前学習
* 事前学習の損失(NLL)

![Screenshot from 2020-11-25 22-34-41](https://user-images.githubusercontent.com/19440811/100234894-0ebbdc00-2f6f-11eb-887c-59e3d6d7a6a4.png)

* Transformer Decoderの式

  ![Screenshot from 2020-11-25 22-38-31](https://user-images.githubusercontent.com/19440811/100234892-0e234580-2f6f-11eb-8370-8707e3c64816.png)

  * 注意１：W_pは、Position Encodingの三角関数のルールベースの式の代わり
  * 注意２：ActivationにGELUを使用（ `sigmoid(1.702 * x) * x` で近似可能）
  
    ![Screenshot from 2020-11-26 01-16-47](https://user-images.githubusercontent.com/19440811/100254043-3027c280-2f85-11eb-8509-7600d656272f.png)
    
　　 ![Screenshot from 2020-11-26 01-19-15](https://user-images.githubusercontent.com/19440811/100254292-77ae4e80-2f85-11eb-8d9b-709531984717.png)



## ファインチューン
* ファインチューンレイヤー

![Screenshot from 2020-11-25 22-38-41](https://user-images.githubusercontent.com/19440811/100234891-0d8aaf00-2f6f-11eb-9c04-91dce1f0f29b.png)

* ファインチューンの損失(NLLであり、cross entropyではない)

![Screenshot from 2020-11-25 22-38-52](https://user-images.githubusercontent.com/19440811/100234887-0cf21880-2f6f-11eb-9bf1-d000c9087e26.png)


* 事前学習の損失関数をファインチューンでも足し算する場合(auxiliary loss)

![Screenshot from 2020-11-26 01-08-51](https://user-images.githubusercontent.com/19440811/100253049-facea500-2f83-11eb-8016-6a212abffdf8.png)


## モデル
* ファインチューンレイヤーは、タスクごとに異なっている。

![Screenshot from 2020-11-25 22-39-42](https://user-images.githubusercontent.com/19440811/100235005-33b04f00-2f6f-11eb-8ac6-9e785a6510bf.png)

## 実験結果
### レイヤー数とZero Shotタスク
* 左側：deepなほど、未知のデータ・セットに対するdevーACCがよくなる
* 右側：事前学習のみのモデルで、当時最良モデル(明示なし)とランダムguess(２値分類なら0.5)に比較して何％上昇したか
  * 点線がLSTMなのでTransformerは良い

![Screenshot from 2020-11-25 22-40-53](https://user-images.githubusercontent.com/19440811/100235110-52164a80-2f6f-11eb-85c6-cf62379e8cdb.png)

### Ablation Study
* 比較項目
  * (auxiliary loss) ファインチューン時点で言語モデルのロス関数を含めて最適化するか否か
  * (pre-training) 事前学習するか否か
  
|表の項目|事前学習の有無|auxiliary lossの有無|transformerの使用の有無|
|----|----|----|----|
|Transformer w/ aux LM(full)|o|o|o|
|Transformer w/o pre-training|x|o|o|
|Transformer w/o aux LM|o|x|o|
|LSTM w/ aux LM|o|o|x|
  
* ファインチューン時点で言語モデルロスは、タスクごとに良し悪しが変化する。
* 事前学習は必ず良くなる


![Screenshot from 2020-11-26 01-26-11](https://user-images.githubusercontent.com/19440811/100255047-66197680-2f86-11eb-9402-da74556948c0.png)
